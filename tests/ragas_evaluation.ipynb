{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ragas Evaluation for Ever Quint RAG System\n",
                "\n",
                "This notebook evaluates the RAG system using Ragas metrics: **Context Precision**, **Faithfulness**, and **Answer Relevancy**.\n",
                "It uses the \"Ever Quint\" content as the ground truth context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ragas.run_config import RunConfig\n",
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "# Ensure backend modules can be imported\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../backend')))\n",
                "\n",
                "from backend.backend.rag_search import initialize_system, answer_query, create_llm, hybrid_retrieve\n",
                "from datasets import Dataset \n",
                "from ragas import evaluate\n",
                "from ragas.metrics import (\n",
                "    faithfulness,\n",
                "    answer_relevancy,\n",
                "    context_precision,\n",
                "    context_recall\n",
                ")\n",
                "# Import Ragas Langchain Wrappers (Available in newer Ragas versions)\n",
                "# Note: Ragas automatically wraps Langchain LLMs if passed directly in recent versions.\n",
                "from langchain_groq import ChatGroq\n",
                "from langchain_huggingface import HuggingFaceEmbeddings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize RAG System and Load Documents\n",
                "We initialize the same system used in the app, which loads the `about_everquint.txt` we just saved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Initializing system...\n",
                        "[INFO] Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
                        "[INFO] Vector store loaded with 4800 existing documents. Skipping ingestion.\n"
                    ]
                }
            ],
            "source": [
                "vector_ret, wiki_ret, prompt, summary_prompt = initialize_system()\n",
                "rag_llm = create_llm()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Test Set (Questions & Ground Truths)\n",
                "We define a set of questions relevant to the Ever Quint documents to test the system."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running RAG Inference...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
                    ]
                }
            ],
            "source": [
                "test_questions = [\n",
                "    \"What is the People app designed for at Perkins & Will?\",\n",
                "    \"What are the key roles needed for the Senior React Developer position?\",\n",
                "    \"Where is Ever Quint Technologies located?\",\n",
                "    \"How does the Sales Pipeline App help with project management?\",\n",
                "    \"What technologies are required for the MERN Stack Developer role?\"\n",
                "]\n",
                "\n",
                "ground_truths = [\n",
                "    [\"The People app is designed to capture all essential details about an individual within a company, integrating employee data with their projects and pursuits to streamline management and improve efficiency.\"],\n",
                "    [\"The Senior React Developer role requires developing and implementing UI components, optimizing performance, managing application state, mentoring the team, ensuring code quality, and collaborating with cross-functional teams.\"],\n",
                "    [\"Ever Quint Technologies is located at Suite #302, Workafella High Street, 431, Anna Salai, Teynampet, Chennai, Tamil Nadu, India – 600018.\"],\n",
                "    [\"The Sales Pipeline App simplifies lead tracking, streamlines pipeline management, and enhances deal closure by integrating data from Excel, PowerBI, and Deltek. It offers filters for smart searching and real-time revenue tracking.\"],\n",
                "    [\"The MERN Stack Developer role requires MongoDB, Express.js, React.js, and Node.js, along with database querying (SQL, Redis, Elastic Search) and message queueing with Kafka.\"]\n",
                "]\n",
                "\n",
                "data_samples = {\n",
                "    'question': [],\n",
                "    'answer': [],\n",
                "    'contexts': [],\n",
                "    'ground_truth': []\n",
                "}\n",
                "\n",
                "# Run Inference\n",
                "print(\"Running RAG Inference...\")\n",
                "for i, q in enumerate(test_questions):\n",
                "    # Retrieve\n",
                "    combined_text, docs = hybrid_retrieve(vector_ret, wiki_ret, q)\n",
                "    # Ragas expects contexts as a list of strings\n",
                "    contexts_list = [d.page_content for d in docs]\n",
                "    \n",
                "    # Generate Answer\n",
                "    ans = answer_query(rag_llm, prompt, q, combined_text)\n",
                "    \n",
                "    # Store\n",
                "    data_samples['question'].append(q)\n",
                "    data_samples['answer'].append(ans)\n",
                "    data_samples['contexts'].append(contexts_list)\n",
                "    data_samples['ground_truth'].append(ground_truths[i][0]) # Ragas dataset needs string for ground_truth column usually, or list of strings?\n",
                "    # Actually 'ground_truth' in older Ragas was list[str], let's stick to list of strings\n",
                "\n",
                "# data_samples['ground_truth'] = ground_truths"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configure Ragas with Groq and HuggingFace\n",
                "We use Groq (LLaMA 3) as the judge LLM and local HuggingFace embeddings for validaton metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[INFO] Use pytorch device_name: mps\n",
                        "[INFO] Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
                    ]
                }
            ],
            "source": [
                "# Wrap Groq for Ragas\n",
                "# Ragas uses Langchain Embeddings/LLM interfaces\n",
                "evaluator_llm = create_llm(model_name=\"llama-3.3-70b-versatile\")\n",
                "\n",
                "evaluator_embeddings = HuggingFaceEmbeddings(\n",
                "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
                ")\n",
                "\n",
                "# Create Dataset\n",
                "rag_dataset = Dataset.from_dict(data_samples)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Evaluation\n",
                "Calculating metrics..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s][INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "Evaluating:   5%|▌         | 1/20 [00:17<05:32, 17.48s/it][INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "Evaluating:  10%|█         | 2/20 [00:23<03:08, 10.45s/it][INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[ERROR] Exception raised in Job[2]: BadRequestError(Error code: 400 - {'error': {'message': \"'n' : number must be at most 1\", 'type': 'invalid_request_error'}})\n",
                        "Evaluating:  15%|█▌        | 3/20 [01:36<11:07, 39.25s/it][INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "Evaluating:  20%|██        | 4/20 [01:39<06:38, 24.93s/it][INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "Evaluating:  25%|██▌       | 5/20 [01:57<05:34, 22.29s/it][INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
                        "[ERROR] Exception raised in Job[5]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jw9dxeate77sagwbz4ybpyj5` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98270, Requested 2541. Please try again in 11m40.704s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
                        "Evaluating:  30%|███       | 6/20 [03:22<10:13, 43.83s/it][INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "Evaluating:  30%|███       | 6/20 [03:41<08:37, 36.97s/it]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrag_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_recall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRunConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/Projects/ever/ai-engineer-assignment/venv/lib/python3.10/site-packages/ragas/_analytics.py:278\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    277\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 278\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "File \u001b[0;32m~/Projects/ever/ai-engineer-assignment/venv/lib/python3.10/site-packages/ragas/evaluation.py:470\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor, allow_nest_asyncio)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Default behavior: use nest_asyncio for backward compatibility (Jupyter notebooks)\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_async_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/ever/ai-engineer-assignment/venv/lib/python3.10/site-packages/ragas/async_utils.py:156\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(async_func, allow_nest_asyncio)\u001b[0m\n\u001b[1;32m    148\u001b[0m     loop_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(loop)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot execute nested async code with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloop_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muvloop does not support nested event loop execution. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use asyncio\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms standard event loop in Jupyter environments, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor refactor your code to avoid nested async calls.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/ever/ai-engineer-assignment/venv/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
                        "File \u001b[0;32m~/Projects/ever/ai-engineer-assignment/venv/lib/python3.10/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
                        "File \u001b[0;32m~/Projects/ever/ai-engineer-assignment/venv/lib/python3.10/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
                        "File \u001b[0;32m~/Projects/ever/ai-engineer-assignment/venv/lib/python3.10/selectors.py:562\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
                        "[ERROR] Exception raised in Job[6]: TimeoutError()\n",
                        "[ERROR] Exception raised in Job[7]: AssertionError(set LLM before use)\n",
                        "[ERROR] Exception raised in Job[8]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[9]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[10]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[11]: AssertionError(set LLM before use)\n",
                        "[ERROR] Exception raised in Job[12]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[13]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[14]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[15]: AssertionError(set LLM before use)\n",
                        "[ERROR] Exception raised in Job[16]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[17]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[18]: AssertionError(LLM is not set)\n",
                        "[ERROR] Exception raised in Job[19]: AssertionError(set LLM before use)\n"
                    ]
                }
            ],
            "source": [
                "results = evaluate(\n",
                "    rag_dataset,\n",
                "    metrics=[\n",
                "        context_precision,\n",
                "        faithfulness,\n",
                "        answer_relevancy,\n",
                "        context_recall,\n",
                "    ],\n",
                "    llm=evaluator_llm,\n",
                "    embeddings=evaluator_embeddings,\n",
                "\n",
                "    run_config=RunConfig(max_workers=1, timeout=120)\n",
                ")\n",
                "\n",
                "print(\"Evaluation Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_input</th>\n",
                            "      <th>retrieved_contexts</th>\n",
                            "      <th>response</th>\n",
                            "      <th>reference</th>\n",
                            "      <th>context_precision</th>\n",
                            "      <th>faithfulness</th>\n",
                            "      <th>answer_relevancy</th>\n",
                            "      <th>context_recall</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>What is the People app designed for at Perkins...</td>\n",
                            "      <td>[The People app is designed to capture all the...</td>\n",
                            "      <td>The People app is designed to capture all the ...</td>\n",
                            "      <td>The People app is designed to capture all esse...</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>0.952722</td>\n",
                            "      <td>NaN</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>What are the key roles needed for the Senior R...</td>\n",
                            "      <td>[Roles and Responsibilities\\nDeveloping and Im...</td>\n",
                            "      <td>The key roles needed for the Senior React Deve...</td>\n",
                            "      <td>The Senior React Developer role requires devel...</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>0.929983</td>\n",
                            "      <td>NaN</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>Where is Ever Quint Technologies located?</td>\n",
                            "      <td>[Logo\\nEver Quint Technologies Private Limited...</td>\n",
                            "      <td>Ever Quint Technologies Private Limited is loc...</td>\n",
                            "      <td>Ever Quint Technologies is located at Suite #3...</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.946970</td>\n",
                            "      <td>1.0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>How does the Sales Pipeline App help with proj...</td>\n",
                            "      <td>[Drive your sales team to success with Sales P...</td>\n",
                            "      <td>The Sales Pipeline App helps with project mana...</td>\n",
                            "      <td>The Sales Pipeline App simplifies lead trackin...</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.837696</td>\n",
                            "      <td>NaN</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>What technologies are required for the MERN St...</td>\n",
                            "      <td>[Identifying new product opportunities and ana...</td>\n",
                            "      <td>The MERN Stack Developer role requires the fol...</td>\n",
                            "      <td>The MERN Stack Developer role requires MongoDB...</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>0.955356</td>\n",
                            "      <td>1.0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                          user_input  \\\n",
                            "0  What is the People app designed for at Perkins...   \n",
                            "1  What are the key roles needed for the Senior R...   \n",
                            "2          Where is Ever Quint Technologies located?   \n",
                            "3  How does the Sales Pipeline App help with proj...   \n",
                            "4  What technologies are required for the MERN St...   \n",
                            "\n",
                            "                                  retrieved_contexts  \\\n",
                            "0  [The People app is designed to capture all the...   \n",
                            "1  [Roles and Responsibilities\\nDeveloping and Im...   \n",
                            "2  [Logo\\nEver Quint Technologies Private Limited...   \n",
                            "3  [Drive your sales team to success with Sales P...   \n",
                            "4  [Identifying new product opportunities and ana...   \n",
                            "\n",
                            "                                            response  \\\n",
                            "0  The People app is designed to capture all the ...   \n",
                            "1  The key roles needed for the Senior React Deve...   \n",
                            "2  Ever Quint Technologies Private Limited is loc...   \n",
                            "3  The Sales Pipeline App helps with project mana...   \n",
                            "4  The MERN Stack Developer role requires the fol...   \n",
                            "\n",
                            "                                           reference  context_precision  \\\n",
                            "0  The People app is designed to capture all esse...                1.0   \n",
                            "1  The Senior React Developer role requires devel...                1.0   \n",
                            "2  Ever Quint Technologies is located at Suite #3...                NaN   \n",
                            "3  The Sales Pipeline App simplifies lead trackin...                NaN   \n",
                            "4  The MERN Stack Developer role requires MongoDB...                1.0   \n",
                            "\n",
                            "   faithfulness  answer_relevancy  context_recall  \n",
                            "0           NaN          0.952722             NaN  \n",
                            "1           NaN          0.929983             NaN  \n",
                            "2           1.0          0.946970             1.0  \n",
                            "3           1.0          0.837696             NaN  \n",
                            "4           1.0          0.955356             1.0  "
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df = results.to_pandas()\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'context_precision': 1.0000, 'faithfulness': 1.0000, 'answer_relevancy': 0.9245, 'context_recall': 1.0000}\n"
                    ]
                }
            ],
            "source": [
                "# Display average scores\n",
                "print(results)\n",
                "df.to_csv(\"ragas_results.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
